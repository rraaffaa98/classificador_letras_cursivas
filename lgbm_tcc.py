# -*- coding: utf-8 -*-
"""LGBM_TCC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B0-d99CjTHQtU5A2Dj6WXGyxXLpsQdxj
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets, metrics
from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

dataset= pd.read_csv('/content/drive/MyDrive/dataset/dataset_A_Z_alfabeto.csv').astype('float32')
dataset.rename(columns={'0':'label'}, inplace=True)

X = dataset.drop('label',axis = 1)
y = dataset['label']

alfabeto_data = np.reshape(X.values, (X.shape[0], 28, 28))

import lightgbm as lgb

!pip install dask[dataframe]

import dask.dataframe as dd

from sklearn.model_selection import train_test_split
from sklearn import metrics
import lightgbm as lgb

# Achatamento de dados
data = alfabeto_data.reshape((alfabeto_data.shape[0], -1))

# Primeira divisão: 20% para treino e 80% restante
X_train, X_resto, y_train, y_resto = train_test_split(data, y, test_size=0.2, random_state=42)

# Segunda divisão: 50% do restante para validação e 50% para teste (25% para cada)
X_validacao, X_test, y_validacao, y_test = train_test_split(X_resto, y_resto, test_size=0.25, random_state=13)

# Verificar a distribuição de classes no conjunto de treino
print(y_train.value_counts())

# Definir o modelo XGBoost (LGBMClassifier)
xgb = lgb.LGBMClassifier()

# Treinar o modelo
xgb.fit(X_train, y_train)

# Fazer previsões no conjunto de teste
predicted_test = xgb.predict(X_test)

# Fazer previsões no conjunto de validação
predicted_validacao = xgb.predict(X_validacao)

# Avaliar o modelo no conjunto de teste
print("Relatório de Classificação (Conjunto de Teste):")
print(metrics.classification_report(y_test, predicted_test))
print("Matriz de Confusão (Conjunto de Teste):")
print(metrics.confusion_matrix(y_test, predicted_test))

# Avaliar o modelo no conjunto de validação
print("Relatório de Classificação (Conjunto de Validação):")
print(metrics.classification_report(y_validacao, predicted_validacao))
print("Matriz de Confusão (Conjunto de Validação):")
print(metrics.confusion_matrix(y_validacao, predicted_validacao))

# Acurácia no conjunto de teste
accuracy_test = xgb.score(X_test, y_test)
print(f'Acurácia no conjunto de teste: {accuracy_test:.2f}')

# Acurácia no conjunto de validação
accuracy_validacao = xgb.score(X_validacao, y_validacao)
print(f'Acurácia no conjunto de validação: {accuracy_validacao:.2f}')

# Exibir algumas predições do conjunto de teste para conferência
print("Predições no conjunto de teste:", predicted_test[:10])

# Exibir algumas predições do conjunto de validação para conferência
print("Predições no conjunto de validação:", predicted_validacao[:10])

# Calcular a acurácia no conjunto de teste
print(f'Acurácia no conjunto de teste: {metrics.accuracy_score(y_test, predicted_test) * 100:.2f}%')

# Calcular a acurácia no conjunto de validação
print(f'Acurácia no conjunto de validação: {metrics.accuracy_score(y_validacao, predicted_validacao) * 100:.2f}%')

# Calcular a matriz de confusão no conjunto de teste
print("Matriz de Confusão (Conjunto de Teste):")
print(metrics.confusion_matrix(y_test, predicted_test))

# Calcular a matriz de confusão no conjunto de validação
print("Matriz de Confusão (Conjunto de Validação):")
print(metrics.confusion_matrix(y_validacao, predicted_validacao))

# Calcular as métricas de classificação para o conjunto de teste
print("Relatório de Classificação (Conjunto de Teste):")
print(metrics.classification_report(y_test, predicted_test))

# Calcular as métricas de classificação para o conjunto de validação
print("Relatório de Classificação (Conjunto de Validação):")
print(metrics.classification_report(y_validacao, predicted_validacao))

# Avaliar o modelo no conjunto de teste
xgb_score_test = xgb.score(X_test, y_test)
print(f'Score no conjunto de teste: {xgb_score_test:.2f}')

# Avaliar o modelo no conjunto de validação
xgb_score_validacao = xgb.score(X_validacao, y_validacao)
print(f'Score no conjunto de validação: {xgb_score_validacao:.2f}')

from sklearn.metrics import classification_report

# Avaliar o modelo no conjunto de teste
print("Relatório de Classificação (Conjunto de Teste):")
print(classification_report(y_test, predicted_test, zero_division=0))

# Avaliar o modelo no conjunto de validação
print("Relatório de Classificação (Conjunto de Validação):")
print(classification_report(y_validacao, predicted_validacao, zero_division=0))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Matriz de confusão para o conjunto de teste
cm_test = confusion_matrix(y_test, predicted_test)

# Criar o gráfico para a matriz de confusão do conjunto de teste
plt.figure(figsize=(10, 7))
sns.set(font_scale=1.4)  # Tamanho da fonte para os labels
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', annot_kws={"size": 16})  # Mapa de calor
plt.title('Matriz de Confusão do Conjunto de Teste')
plt.xlabel('Rótulo Previsto')
plt.ylabel('Rótulo Real')
plt.show()

# Matriz de confusão para o conjunto de validação
cm_validacao = confusion_matrix(y_validacao, predicted_validacao)

# Criar o gráfico para a matriz de confusão do conjunto de validação
plt.figure(figsize=(10, 7))
sns.set(font_scale=1.4)  # Tamanho da fonte para os labels
sns.heatmap(cm_validacao, annot=True, fmt='d', cmap='Greens', annot_kws={"size": 16})  # Mapa de calor
plt.title('Matriz de Confusão do Conjunto de Validação')
plt.xlabel('Rótulo Previsto')
plt.ylabel('Rótulo Real')
plt.show()

from sklearn import metrics
import matplotlib.pyplot as plt
import numpy as np

# Concatenar os resultados de teste e validação
y_combined = np.concatenate([y_test, y_validacao])
predicted_combined = np.concatenate([predicted_test, predicted_validacao])

# Calcular a matriz de confusão combinada
disp_combined = metrics.ConfusionMatrixDisplay.from_predictions(y_combined, predicted_combined)
disp_combined.figure_.suptitle("Matriz de Confusão - Conjunto de Teste e Validação Combinados")
print(f"Matriz de confusão combinada:\n{disp_combined.confusion_matrix}")

# Exibir a matriz de confusão
plt.show()

print(
    f"Classification report for classifier {clf}:\n"
    f"{metrics.classification_report(y_test, predicted)}\n"
)

from sklearn import metrics
import numpy as np

# Supondo que clf seja o classificador treinado, e os conjuntos de dados já estejam divididos e predições feitas

# Concatenar y_test e y_validacao para uma avaliação conjunta
y_combined = np.concatenate([y_test, y_validacao])
predicted_combined = np.concatenate([predicted_test, predicted_validacao])

# Exibir algumas predições para conferir
print(predicted_combined[:10])

# Calcular a acurácia combinada
accuracy_combined = metrics.accuracy_score(y_combined, predicted_combined)
print("Acurácia (teste e validação combinados):", accuracy_combined)

# Calcular a matriz de confusão combinada
confusion_matrix_combined = metrics.confusion_matrix(y_combined, predicted_combined)
print("Matriz de Confusão (teste e validação combinados):")
print(confusion_matrix_combined)

# Calcular as métricas de classificação combinadas
classification_report_combined = metrics.classification_report(y_combined, predicted_combined, zero_division=0)
print("Relatório de Classificação (teste e validação combinados):")
print(classification_report_combined)

# Avaliar separadamente os scores
clf_score_test = xgb.score(X_test, y_test)
print("Score no conjunto de teste:", clf_score_test)

clf_score_validacao = xgb.score(X_validacao, y_validacao)
print("Score no conjunto de validação:", clf_score_validacao)